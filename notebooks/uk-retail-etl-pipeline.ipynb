{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a428fc-8bad-4f36-aab8-9491bb9ad3c2",
   "metadata": {},
   "source": [
    "## Project overview: UK retail ETL pipeline & analytics\n",
    "\n",
    "**Project title:**    UK Supermarket Retail ETL and Analytics Pipeline\n",
    "\n",
    "### Project Context:\n",
    "The UK retail sector generates large volumes of product pricing and sales data daily across multiple supermarkets, including Tesco, ASDA, Aldi, Morrisons, and Sainsbury. Analysts and decision-makers need consolidated, clean, and structured data to monitor price trends, track inflation, perform competitor analysis, and optimize pricing strategies. Currently, raw data exists as large CSV files, which are inconsistent in format and too big for manual processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34026649-c8dd-4c30-82ab-25f7189ead21",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives\n",
    "\n",
    "- **Data staging & cleaning:**  \n",
    "  Load raw CSVs into a staging database, standardize column names, handle missing values, and remove duplicates.\n",
    "\n",
    "- **Dimension tables:**  \n",
    "  Build `dim_supermarket`, `dim_product`, and `dim_date` for consistent analytics.\n",
    "\n",
    "- **Fact tables:**  \n",
    "  Build `fact_prices` and `fact_sales` to capture daily prices and sales transactions.\n",
    "\n",
    "- **Aggregates & analytics:**  \n",
    "  Create summary tables like `agg_daily_prices` to facilitate reporting, KPI tracking, and trend analysis.\n",
    "\n",
    "- **Visualization:**  \n",
    "  Connect **Power BI** to the analytics database for dashboards tracking price trends, inflation, and competitor performance.\n",
    "\n",
    "- **Version control & documentation:**  \n",
    "  Maintain ETL scripts, SQL queries, and notebooks in **GitHub** for reproducibility and collaboration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4da117-cf0c-4c69-8ebe-88994a24dc00",
   "metadata": {},
   "source": [
    "## Key business questions\n",
    "\n",
    "This project consolidates and analyzes UK supermarket pricing data from multiple sources to enable data-driven decision making. Below are the main **business questions** this project addresses:\n",
    "\n",
    "## 1. Price trends over time\n",
    "- How do average product prices change over time across UK supermarkets?\n",
    "- Are there seasonal or weekly patterns in product pricing?\n",
    "- Which supermarkets show the most price stability or volatility?\n",
    "\n",
    "## 2. Supermarket comparison\n",
    "- How do prices for the same products compare across different supermarket chains?\n",
    "- Which supermarket offers the lowest or highest prices for a given product category?\n",
    "- Are discount supermarkets consistently cheaper than traditional supermarkets?\n",
    "\n",
    "## 3. Product-level insights\n",
    "- Which products experience the most price fluctuations?\n",
    "- Are there categories of products that drive overall price changes?\n",
    "- How many price observations are recorded per product, per supermarket, per day?\n",
    "\n",
    "## 4. Inflation & market monitoring\n",
    "- Can we track average price inflation for key grocery items over time?\n",
    "- How does inflation differ across store types (Superstore vs Discount)?\n",
    "- Which products or supermarkets are contributing most to price increases?\n",
    "\n",
    "## 5. Data quality & completeness\n",
    "- Are there any outliers or inconsistent data points in pricing?\n",
    "- How complete is the dataset across products, supermarkets, and dates?\n",
    "- How can we ensure data is clean and standardized for reliable analytics?\n",
    "\n",
    "## 6. Analytics & dashboard readiness\n",
    "- How can clean, aggregated data be presented effectively in Power BI?\n",
    "- What visualizations best communicate trends, comparisons, and KPIs to stakeholders?\n",
    "- How can the ETL pipeline be structured to support daily or incremental updates?\n",
    "\n",
    "\n",
    "\n",
    "> **Summary:**  \n",
    "This project transforms raw UK retail data into a clean, structured, and analytics-ready format. It enables stakeholders to answer operational, strategic, and competitive questions related to pricing trends, supermarket comparison, product-level insights, and market monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f98ab-a880-4e99-bcb9-1adbfac908e9",
   "metadata": {},
   "source": [
    "### Project goal:\n",
    "Develop an end-to-end ETL pipeline to transform raw retail data into clean, structured, and analytics-ready tables. Build supporting dimension and fact tables to enable advanced analytics and dashboards using Power BI, while orchestrating the workflow with Airflow. The pipeline should be scalable, reproducible, and follow best practices for UK retail data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ec49e-8ec1-4b57-9a03-469c4c5e8b8f",
   "metadata": {},
   "source": [
    "## Tools & technologies\n",
    "\n",
    "- **Data processing & ETL:**  \n",
    "  Python, Jupyter Notebook, pandas, SQLAlchemy\n",
    "\n",
    "- **Database:**  \n",
    "  MySQL (staging + analytics)\n",
    "\n",
    "- **Visualization:**  \n",
    "  Power BI\n",
    "\n",
    "- **Version control:**  \n",
    "  GitHub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551e08f-c1a7-45b1-8e10-18fab05a4705",
   "metadata": {},
   "source": [
    "### Expected Outcomes:\n",
    "\n",
    "- A clean, standardized, and queryable dataset for UK retail prices.\n",
    "\n",
    "- Robust dimension and fact tables supporting analytics and dashboards.\n",
    "\n",
    "- Automated ETL workflow enabling daily or incremental updates.\n",
    "\n",
    "- Interactive Power BI dashboards for business insights on price trends, inflation, and competitor pricing.\n",
    "\n",
    "- A reproducible, portfolio-ready ETL project demonstrating best practices in retail data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd45f5-47af-4385-a11e-6fd5d7284887",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5f57a-2081-4f33-b920-b1c4e0ad5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35328627-8e4c-496d-9fe8-373cd15cdc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of supermarkets: 5\n",
      "Number of products: 200\n",
      "Number of dates: 90\n",
      "Sample products: ['Product_001', 'Product_002', 'Product_003', 'Product_004', 'Product_005']\n",
      "Sample dates: [datetime.datetime(2026, 1, 1, 0, 0), datetime.datetime(2026, 1, 2, 0, 0), datetime.datetime(2026, 1, 3, 0, 0), datetime.datetime(2026, 1, 4, 0, 0), datetime.datetime(2026, 1, 5, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define supermarkets, products, and dates\n",
    "\n",
    "# 5 UK supermarkets\n",
    "supermarkets = [\"Tesco\", \"ASDA\", \"Aldi\", \"Morrisons\", \"Sainsbury\"]\n",
    "\n",
    "# 200 products: Product_001, Product_002, ..., Product_200\n",
    "products = [f\"Product_{str(i).zfill(3)}\" for i in range(1, 201)]\n",
    "\n",
    "# 90 days starting from 1st Jan 2026\n",
    "start_date = datetime(2026, 1, 1)\n",
    "dates = [start_date + timedelta(days=i) for i in range(90)]\n",
    "\n",
    "# Quick check\n",
    "print(\"Number of supermarkets:\", len(supermarkets))\n",
    "print(\"Number of products:\", len(products))\n",
    "print(\"Number of dates:\", len(dates))\n",
    "print(\"Sample products:\", products[:5])\n",
    "print(\"Sample dates:\", dates[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab08c10-7193-4b81-83a9-f3bed9427e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reason for using generated data:\n",
    "\n",
    "I am using generated data because the publicly available Kaggle dataset had over 90 million rows, which MySQL could not handle on my local setup. It kept crashing. By generating a smaller, realistic dataset, I retain the same information and structure while ensuring it is manageable for testing and development of the ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1482f4-3fa0-4f7a-abe5-261eec7fcca4",
   "metadata": {},
   "source": [
    "### Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8b8ef-2f9b-447e-8b1f-334cce0d9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb255a46-f8ca-4117-b9ee-f03fc32c74ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset:\n",
      "   fact_id    raw_date supermarket product_name  price_gbp\n",
      "0  F000000  2026-01-01       Tesco  Product_001      26.18\n",
      "1  F000001  2026-01-01       Tesco  Product_001      17.96\n",
      "2  F000002  2026-01-01       Tesco  Product_001      42.39\n",
      "3  F000003  2026-01-01       Tesco  Product_002      15.96\n",
      "4  F000004  2026-01-01       Tesco  Product_002       9.06\n",
      "Total rows: 179976\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate price observations\n",
    "\n",
    "rows = []\n",
    "\n",
    "for date in dates:\n",
    "    for supermarket in supermarkets:\n",
    "        for product in products:\n",
    "            # 1-3 price observations per product per supermarket per day\n",
    "            for _ in range(np.random.randint(1, 4)):\n",
    "                price = round(np.random.uniform(0.5, 50), 2)  # realistic price in GBP\n",
    "                rows.append([date.strftime(\"%Y-%m-%d\"), supermarket, product, price])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"raw_date\", \"supermarket\", \"product_name\", \"price_gbp\"])\n",
    "\n",
    "# Add a synthetic fact_id\n",
    "df[\"fact_id\"] = [\"F\" + str(i).zfill(6) for i in range(len(df))]\n",
    "\n",
    "# Reorder columns\n",
    "df = df[[\"fact_id\", \"raw_date\", \"supermarket\", \"product_name\", \"price_gbp\"]]\n",
    "\n",
    "# Quick check\n",
    "print(\"Sample dataset:\")\n",
    "print(df.head())\n",
    "print(\"Total rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904414c7-8841-4f71-a86c-1f1230ec1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the dataset into my raw project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8540b56-a593-4ded-95fb-7a4756ac27f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to C:\\Users\\ASUS-PC\\OneDrive\\Desktop\\uk-retail-etl-pipeline\\data\\raw\\retail_sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = r\"C:\\Users\\ASUS-PC\\OneDrive\\Desktop\\uk-retail-etl-pipeline\\data\\raw\\retail_sample_data.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff071ff-1d7c-425a-baaa-1edd7fbdd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next Step: Load CSV into MySQL staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65154e3-1096-4ac3-9c79-ce8bcaa06ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded successfully. Number of rows: 179976\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Load CSV into MySQL staging table\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# MySQL connection string\n",
    "# Format: mysql+mysqlconnector://username:password@host:port/database\n",
    "engine = create_engine('mysql+mysqlconnector://root:root@localhost:3306/staging')\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = r\"C:\\Users\\ASUS-PC\\OneDrive\\Desktop\\uk-retail-etl-pipeline\\data\\raw\\retail_sample_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"CSV loaded successfully. Number of rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbfe5f24-4984-45d7-85e6-8af20ca3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection string for root user with no password\n",
    "engine = create_engine('mysql+mysqlconnector://root@localhost:3306/staging')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817c2a2f-b975-46bf-97a8-a6cba1e21062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into staging.stg_retail_prices successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = r\"C:\\Users\\ASUS-PC\\OneDrive\\Desktop\\uk-retail-etl-pipeline\\data\\raw\\retail_sample_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load into MySQL\n",
    "df.to_sql('stg_retail_prices', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Data loaded into staging.stg_retail_prices successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d365f-6b55-45a6-b792-45dbfbf86b54",
   "metadata": {},
   "source": [
    "## Transformation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0812e-c3fb-43a2-b06e-bf89758e7be3",
   "metadata": {},
   "source": [
    "#### Analytics database + dimension tables in MYSQL\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "1️⃣ Create an analytics database\n",
    "\n",
    "2️⃣ Clean the staging data\n",
    "\n",
    "3️⃣ Build dimension tables\n",
    "\n",
    "4️⃣ Remove duplicates / bad records\n",
    "\n",
    "5️⃣ Prepare data for fact table later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7ad2b-6396-41d8-91f7-29949fcb6dfc",
   "metadata": {},
   "source": [
    "### The next step in MYSQL is to build fact table\n",
    "\n",
    "##### Goal\n",
    "\n",
    "  The fact table will:\n",
    "\n",
    "- Connect all dimension tables via foreign keys\n",
    "\n",
    "- Store metrics like price_gbp\n",
    "\n",
    "- Be analytics-ready for Power BI dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef299776-a66a-4d77-92c3-801b785b6a30",
   "metadata": {},
   "source": [
    "## Load (Analytics layer)\n",
    "-  Load transformed data into analytics schema.\n",
    "\n",
    "\n",
    "scripts\n",
    "│\n",
    "\n",
    "├── extract_generate_data.py\n",
    "\n",
    "├── load_to_staging.py   \n",
    "\n",
    "├─ transform_clean.py\n",
    "\n",
    "├── load_to_analytics.py \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2fcd6-cce5-4280-bebc-a48c43f3fd50",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "  - Connect Power BI to analytics database.\n",
    "\n",
    "    Power BI dashboards are built on the analytics schema populated through automated load processes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18808b6-42b1-4407-8a7d-237698adf8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
